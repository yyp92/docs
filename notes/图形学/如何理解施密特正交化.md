# 如何理解施密特正交化

> **施密特正交化我们在之前就写过，不过当时给出的证明方法比较复杂，这一版对其进行了简化**

如果$\vec{x_1},\vec{x_2},...,\vec{x_n}$是某向量空间的基，那么可通过下列做法找到该向量空间中的个两两正交的向量$\vec{x_1},\vec{x_2},...,\vec{x_n}$：

$$
\vec{x_1},\vec{x_2},...,\vec{x_n} \underrightarrow{施密特正交化}
\begin{cases}
\vec{v_1} = \vec{x_1} \\
\vec{v_2} = \vec{x_2} - \frac{\vec{x_2}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} \\
\vec{v_3} = \vec{x_3} - \frac{\vec{x_3}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} - \frac{\vec{x_3}·\vec{v_2}}{\vec{v_2}·\vec{v_2}}\vec{v_2} \\
\cdots \\
\vec{v_n} = \vec{x_n} - \frac{\vec{x_n}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}
\vec{v_1}- \cdots - \frac{\vec{x_n}·\vec{v_{n-1}}}{\vec{v_{n-1}}·\vec{v_{n-1}}}\vec{v_{n-1}} \\
\end{cases}
$$

该方法称为施密特正交化（Gram–Schmidt process）。

施密特正交化的几何意义是，比如已知 $R^3$ 中的某向量空间（下图中的蓝色平面）的基为 $\vec{x_1},\vec{x_2}$ :

![](../../\images\schmidt-orthogonalization-2.jpg)

那么通过施密特正交化，可借助 $\vec{x_1},\vec{x_2}$ 得到 $\vec{v_1},\vec{v_2}$ ，$\vec{v_1},\vec{v_2}$  就是该向量空间的一个正交基：

$$
\vec{x_1},\vec{x_2} \underrightarrow{施密特正交化}
\begin{cases}
\vec{v_1} = \vec{x_1} \\
\vec{v_2} = \vec{x_2} - \frac{\vec{x_2}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} \\
\end{cases}
$$

![](../../\images\schmidt-orthogonalization-3.jpg)

下面来解释下施密特正交化是如何推导出来的。

## 二维平面

先来讲解下如何寻找二维向量空间。

### 思路

先从特殊的二维向量空间 $R^2$ 说起。比如知道 $R^2$ 的一组基，也就是下图中的两个向量：

![](../../\images\schmidt-orthogonalization-4.jpg)

只要将其中一个向量对另外一个向量进行投影，就可以得到 $R^2$ 的正交基：

![](../../\images\schmidt-orthogonalization-5.gif)

### 代数

下面来进行代数推导，假设基为 $\vec{x_1},\vec{x_2}$ ：

![](../../\images\schmidt-orthogonalization-6.jpg)

任选其一作为 $\vec{v_1}$ ，比如选 $\vec{x_1}$ ：

![](../../\images\schmidt-orthogonalization-7.gif)

作出 $\vec{x_2}$ 在 $\vec{v_1}$ 上的投影 $\vec{x_2}$ ，其垂线向量 $\vec{x_2} - \bar{x_2}$ 就是要求的 $\vec{v_2}$ ，即 $\vec{v_2} = \vec{x_2} - \bar{x_2}$ ：

![](../../\images\schmidt-orthogonalization-8.jpg)

因为 $\vec{x_2}、\vec{v_2}$ 和 $\bar{x_2}$ 构成三角形，所以根据向量减法的几何意义有:

$$
\vec{v_2} = \vec{x_2} - \bar{x_2}
$$

又投影 $\bar{x_2}$ 和 $\vec{v_1}$ 在一条直线上，两者线性相关，所以可假设:

$$
\bar{x_2} = k_1\vec{v_1}
$$

因此：

$$
\vec{v_2} = \vec{x_2} - \bar{x_2} = \vec{x_2} - k_1\vec{v_1}
$$

![](../../\images\schmidt-orthogonalization-9.jpg)

因为 $\vec{v_2}$ 和 $\vec{v_1}$ 正交，所以：

$$
\vec{v_2} \cdot \vec{v_1} = 0 \\
\implies (\vec{x_2} - k_1\vec{v_1}) \cdot \vec{v_1} = 0 \\
\implies \vec{x_2} \cdot \vec{v_1} - k_1\vec{v_1} \cdot \vec{v_1} = 0 \\
\implies k_1 = \frac{\vec{x_2} \cdot \vec{v_1}}{\vec{v_1} \cdot \vec{v_1}}
$$

所以：

$$
\vec{v_2} = \vec{x_2} - k_1\cdot\vec{v_1} = \vec{x_2} - \frac{\vec{x_2}\cdot\vec{v_1}}{\vec{v_1}\cdot\vec{v_1}}\vec{v_1}
$$

这样就得到了 $R^2$ 的一组正交基 $\vec{v_1},\vec{v_2}$ ：

![](../../\images\schmidt-orthogonalization-10.jpg)

### 总结

上述方法就是二维空间中的施密特正交化，可以总结如下：

$$
\vec{x_1},\vec{x_2} \underrightarrow{施密特正交化} 
\begin{cases}
\vec{v_1} = \vec{x_1} \\
\vec{v_2} = \vec{x_2} - \frac{\vec{x_2}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} \\
\end{cases}
$$

上述推导过程并没有被限制在 $R^2$ 中，所以它也可以完成开头提到的在三维空间中的平面上寻找正交基的任务：

![](../../\images\schmidt-orthogonalization-11.jpg)

## 三维立体

再来看看如何寻找三维向量空间的正交基。

### 思路

还是以特殊的三维向量空间 $R^3$ 为例。比如知道 $R^3$ 的一组基，也就是下图中的三个向量：

![](../../\images\schmidt-orthogonalization-12.jpg)

先按照二维平面的方法，将其中任意两个向量正交化：

![](../../\images\schmidt-orthogonalization-13.gif)

然后向这两个正交向量的张成空间作垂线，从而得到三个正交向量，也就是 $R^3$ 的一组正交基：

![](../../\images\schmidt-orthogonalization-14.gif)

### 代数

下面来进行代数推导，假设基为 $\vec{x_1},\vec{x_2},\vec{x_3}$ ：

![](../../\images\schmidt-orthogonalization-15.jpg)

任选两个向量，按照上一节介绍的方法将其中任意两个向量正交化，得到 $\vec{v_1}$ 和 $\vec{v_2}$ ：

$$
\vec{x_1},\vec{x_2} \underrightarrow{施密特正交化} 
\begin{cases}
\vec{v_1} = \vec{x_1} \\
\vec{v_2} = \vec{x_2} - \frac{\vec{x_2}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} \\
\end{cases}
$$

![](../../\images\schmidt-orthogonalization-16.jpg)

作出${\vec{x_3}}$ 在 $\vec{v_1},\vec{v_2}$ [张成平面](https://www.matongxue.com/parts/2227/)上的投影 ${\bar{x_3}}$ ，连接 $\vec{x_3}$ 和 ${\bar{x_3}}$ 就得到要求的垂线向量 $\vec{v_3}$ ：

![](../../\images\schmidt-orthogonalization-17.jpg)

因为 $\vec{x_3}、\vec{v_3}$ 和 $\bar{x_3}$ 构成三角形，所以根据向量减法的几何意义有

$$
\vec{v_3} = \vec{x_3} - \bar{x_3}
$$

又投影 $\bar{x_3}$ 在 $\vec{v_1},\vec{v_2}$ 的张成平面上，所以 $\bar{x_3}$ 是 $\vec{v_1},\vec{v_2}$ 的线性组合，可假设

$$
\bar{x_3} = k_1\vec{v_1} + k_2\vec{v_2}
$$

因此：

$$
\vec{v_3} = \vec{x_3} - \bar{x_3} = \vec{x_3} - k_1\vec{v_1} - k_2\vec{v_2}
$$

![](../../\images\schmidt-orthogonalization-18.jpg)

因为 $\vec{v_3}$ 垂直于 $\vec{v_1},\vec{v_2}$ 的张成平面，所以 $\vec{v_3}$ 必然垂直于 $\vec{v_1}$ 和 $\vec{v_2}$ ，所以有：

$$
\begin{cases}
\vec{v_3} \cdot \vec{v_1} = (\vec{x_3} - k_1\vec{v_1} - k_2\vec{v_2})\cdot\vec{v_1} = 0 \\
\vec{v_3} \cdot \vec{v_2} = (\vec{x_3} - k_1\vec{v_1} - k_2\vec{v_2})\cdot\vec{v_2} = 0 
\end{cases}
$$

注意到 $\vec{v_1}$ 和 ${\vec{v_2}}$ 正交，即有 $\vec{v_1} \cdot \vec{v_2} = 0$ ，根据上面的方程组可以分别推出：

$$
(\vec{x_3} - k_1\vec{v_1} - k_2\vec{v_2})\cdot\vec{v_1} = 0 \\
\implies \vec{x_3} \cdot \vec{v_1} - k_1\vec{v_1}\cdot\vec{v_1} - k_2\vec{v_2}\cdot\vec{v_1} = 0 \\
\implies \vec{x_3}\cdot\vec{v_1} - k_1\vec{v_1}\cdot\vec{v_1} = 0 \\
\implies k_1 = \frac{\vec{x_3}\cdot\vec{v_1}}{\vec{v_1}\cdot\vec{v_1}}
$$

$$
(\vec{x_3} - k_1\vec{v_1} - k_2\vec{v_2})\cdot\vec{v_2} = 0 \\
\implies \vec{x_3} \cdot \vec{v_2} - k_1\vec{v_1}\cdot\vec{v_2} - k_2\vec{v_2}\cdot\vec{v_2} = 0 \\
\implies \vec{x_3}\cdot\vec{v_2} - k_1\vec{v_2}\cdot\vec{v_2} = 0 \\
\implies k_2 = \frac{\vec{x_3}\cdot\vec{v_2}}{\vec{v_2}\cdot\vec{v_2}}
$$

所以：

$$
\vec{v_3} = \vec{x_3} - k_1\vec{v_1} - k_2\vec{v_2} = \vec{x_3} - \frac{\vec{x_3}\cdot\vec{v_1}}{\vec{v_1}\cdot\vec{v_1}}\vec{v_1}- \frac{\vec{x_3}\cdot\vec{v_2}}{\vec{v_2}\cdot\vec{v_2}}\vec{v_2}
$$

$$
\vec{v_3} = \vec{x_3} - \frac{\vec{x_3}\cdot\vec{v_1}}{\vec{v_1}\cdot\vec{v_1}}\vec{v_1}- \frac{\vec{x_3}\cdot\vec{v_2}}{\vec{v_2}\cdot\vec{v_2}}\vec{v_2}
$$

这样就得到了 $R^3$ 的一组正交基 $\vec{v_1},\vec{v_2},\vec{v_3}$ ：

![](../../\images\schmidt-orthogonalization-19.jpg)

### 总结

上述方法就是三维空间中的施密特正交化，可以总结如下：

$$
\vec{x_1},\vec{x_2},\vec{x_3} \underrightarrow{施密特正交化}
\begin{cases}
\vec{v_1} = \vec{x_1} \\
\vec{v_2} = \vec{x_2} - \frac{\vec{x_2}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} \\
\vec{v_3} = \vec{x_3} - \frac{\vec{x_3}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} - \frac{\vec{x_3}·\vec{v_2}}{\vec{v_2}·\vec{v_2}}\vec{v_2} \\
\end{cases}
$$

**更高维度**

更高维度的情况以此类推,从而得到:

$$
\vec{x_1},...,\vec{x_n} \underrightarrow{施密特正交化}
\begin{cases}
\vec{v_1} = \vec{x_1} \\
\vec{v_2} = \vec{x_2} - \frac{\vec{x_2}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} \\
\vec{v_3} = \vec{x_3} - \frac{\vec{x_3}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}\vec{v_1} - \frac{\vec{x_3}·\vec{v_2}}{\vec{v_2}·\vec{v_2}}\vec{v_2} \\
\cdots \\
\vec{v_n} = \vec{x_n} - \frac{\vec{x_n}·\vec{v_1}}{\vec{v_1}·\vec{v_1}}
\vec{v_1}- \cdots - \frac{\vec{x_n}·\vec{v_{n-1}}}{\vec{v_{n-1}}·\vec{v_{n-1}}}\vec{v_{n-1}} \\
\end{cases}
$$
